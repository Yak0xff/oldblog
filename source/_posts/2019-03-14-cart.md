---
layout: post
author: Robin
title: 什么是决策树
tags: 机器学习
categories:
- 机器学习
cover: '/images/cart/cover.jpg'
---

决策树(Decision Tree）是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。在机器学习中，决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。Entropy = 系统的凌乱程度，使用算法ID3, C4.5和C5.0生成树算法使用熵。这一度量是基于信息学理论中熵的概念。
决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。
分类树（决策树）是一种十分常用的分类方法。他是一种监管学习，所谓监管学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。

## 什么是决策树？

有的人可能听过一个词：**CART**，这个代表的意思是**Classification And Regression Tree**。它是一个分类和回归的决策树。它被分为两类，一类是**分类决策树**(Classification Trees)，另一个类是**回归决策树**(Regression Trees)。也就是我们要用这个决策树解决两类问题，一个分类问题一个回归问题。

![](/images/cart/cart.png)

对于分类决策树，一般来说用于一些分类离散的数据，比如说人的性别是男或者女，水果的种类有苹果梨子等等都是离散的。反之回归决策树，那么对应的场景就是连续的数据，比如人的年龄或者室外的温度。当我们进行分类问题时，分类的组之间是无序的。这里首先介绍下什么是有序，可以举个例子比如年龄，又年龄大或者年龄小。那么对于性别问题，男或女，它是没有顺序的。本文要讲的是分类问题在决策树上的应用。

来看个例子，在一个二维平面上有两个颜色分组的数据，我们要用决策树算法来构建分类器。这里的决策树算法要做的事情就是不断用水平和竖直的线不断对平面进行分隔，直到某一个区域类只有红类或者绿类。如图所示，我们画出几条线对平面进行分隔。

![](/images/cart/cart-split.png)

这样图中的红组和蓝组的数据点就被这些数据分隔开来了，但这组数据是为了方便展示而特地画成这个样子的，实际情况并不一定会出现这种比较清晰的分割线。那我们先看看第一条分割线，将其分割成了上下两块区域，虽然两边都是既有红色又有蓝色，但我们可以说分类的结果还是比较纯的。用复杂点的数学语言来说就是，我们正在寻找一条分隔线，可以是水平的也可以是竖直的，我们想要做一个优化的问题，需要最小化分隔后的基尼不纯度。什么叫纯，指的是分隔后的一边如果只有红点或者绿点，那么可以说这个分隔的结果是非常纯的，那么如果两边既有红也有蓝，那么就是不纯的。我们希望当我们添加一条分割线后，想要将两边的纯度和最小化。那么每一条的分割线的寻找实际上就是在做一个优化的问题，那么优化的对象可以是基尼不纯度，也可以是信息学中的熵。这里不做过多解释，只是展示下决策树是如果运作的。

画出第一条分隔线后如图可以得到两组分类结果，一个是x2小于或者不小于60，再然后我们画出第二条分割线，看出x1<50是绿组，否则就是红组，接着再画出第三条分割线，x1<70都是红组，再对x1>70分隔，得出红组和绿组数据。

![](/images/cart/cart-tree.png)

如图其实就是上述所说的工作流程，我们得到的每一片叶子都是比较纯的结果，如果在实际实际生活中，数据可能非常复杂，那么我们的树可能就非常非常大，枝节非常非常多。那么有的时候，有的枝节不一定非要到最后知道yes or no，也许可能在前面某个枝节就停止了。比如对于x2<20这里不再继续分割，假设有个新的数据点落在了这个区域，它落在绿色的区域的概率比落在红色的概率要大，那么我们就可以把这一部分都划分到绿色组中，也就是说可以剪掉多余的枝节，也许它对于训练集是有意义的，但对于更多其他的数据来说，它可能就是个噪音，我们不需要知道这么详细的信息。那最终就没有这两片叶子，到前面一步就结束了。

决策树算法是个很经典的机器学习算法，很多年以前是比较流行的。但到了20世纪初已经逐渐被其他算法所取代。直到最近又发现这个算法中一些新的精妙的东西，比如说随机森林，就是以决策树为根本来展开的。还有提升梯度(Gradient Boosting)等等都是在决策树算法之上我们加上了一些新的元素。

代码实现
由于这次决策树算法，我们没有使用欧式距离，也就是说可以不用进行特征缩放。但最终画图像时之前模版中定义的步距可能就过大或者过小，所以这里就妥协一下保留特征缩放的代码。分类器改成决策树算法的DecisionTreeClassifier。这个方法的参数criterion指的就是标准，默认gini，即基尼指数或者说基尼不纯度。它和熵都是表示分类时划分质量的好坏。这里我们使用熵。其他的参数暂时用不到,random_state依然只是用来大家如果想得到相同的结果时就设置为相同的值即可。

```python
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion='entropy',random_state=)
classifier.fit(X_train, y_train)
```

## 随机森林

在开始讲随机森林之前我们先讲一个更为广义的概念: Ensemble Learning，集成学习。它的意思是我们使用多个分类器对我们的结果进行预测，最后再对分类结果进行一个组合，以达到最终的结果。这个组合的方式有很多种，比如平均，加权平均或者投票等等。这个集成学习的作用就是，我们觉得任何一个单独的分类器去分类结果回感觉有误差，这时可以用成百上千个分类器都进行预测，然后再对结果进行一个组合，可以减少预测结果的浮动率。下面来看看随机森林算法的步骤。

首先，随机采用训练集合中的数据，相当于装袋的过程，构建自己新的训练集；然后用这些数据训练决策树分类器；再然后实际上就是重复第一第二步，但每一次得到的结果是不同的，因为在第一步中我们取得的数据都是随机的。对于一个新的数据点，我们用已经训练好的多个训练器分别对这个新数据的分类进行预测，最后进行一个投票，拥有最大投票数量的分类结果胜出就使用这个分类结果。

![](/images/cart/random-forest.png)

前文讲述了如何构建一棵决策树，现在拥有成百上千棵决策树来帮助我们解决分类问题。这个分类算法还有不少数学上的一些细节问题，比如Boosting(提升)，还有当我们有高维度的情况时，我们每次选取数据时可能只选取部分维度，这样可以避免个别维度比其他维度大的多情况。

## 代码实现

这里依然开始先套用分类的模版，然后换成随机森林分类器，这里的参数n_estimators指的是决策树的数量,这里暂时设置成10 criterion依然设置为entropy。

```python
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)
classifier.fit(X_train, y_train)
```

通过结果观察，这里使用随机森林分类器是会出现过拟合的情况。对比这几篇文章中的分类器，实际上最适合的是核svm和朴素贝叶斯，线性分类器准确度不够，随机森林分类器会出现过拟合，而这两者它们保证了拟合的准确率，并且也不会出现过拟合的问题。

